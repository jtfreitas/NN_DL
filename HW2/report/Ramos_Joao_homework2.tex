% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{multicol,caption}
\usepackage{wrapfig}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{amsmath}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents

\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\newcommand{\squeezeup}{\vspace{-25pt}}

%%% Object appearance
\usepackage{float, makecell}
\usepackage{subcaption}
\newenvironment{Figure}{\par\medskip\noindent\minipage{\linewidth}}{\endminipage\par\medskip}
\newenvironment{Table}{\par\smallskip\noindent\minipage{\linewidth}}{\endminipage\par\smallskip}
\usepackage[linewidth=0.01pt,linecolor=black]{mdframed}
\captionsetup{justification=raggedright,singlelinecheck=false}

%%% END Article customizations

%%% Document content

\title{Homework 1 : Unsupervised Deep Learning}
\author{João Ramos da Silva Tabuaço Freitas \\ Università Degli Studi di Padova \\ Student ID: 2009440}
\date{18 July 2022} %Display a given date

\begin{document}
\maketitle

\section*{\centering Introduction}
Unsupervised learning takes an agnostic approach in learning features in data. This distinguishes it from supervised learning, where models attempt to form a connection between an input and a ground-truth target. The performance of unsupervised learning models is normally assessed by how faithfully the data can be reproduced from the features learned.

\textbf{Autoencoders} and \textbf{generative-adversarial networks} (GANs) are two models used in unsupervised learning. In both cases, these models are composed by a perception module along with a reproduction module. These two work in tandem to achieve faithful reproduction of data with large number of features.

\noindent The goal of the autoencoder is to study inputs and tune itself to embed input data in a latent space of reduced dimensionality, and subsequently reproduce it. To achieve this, an encoder transforms an input into a single vector in said latent space. A decoder then takes in the latent vector output of the encoder and attempts to retrieve the initial input as faithfully as possible. This motivates the use of mean-squared error (MSE) to quantify the deviation, with the ultimate goal of minimizing it.

\noindent Autoencoders may be undercomplete or overcomplete, depending on whether they have a smaller, or larger number of hidden neurons than the input dimensions. The former performs dimensionality reduction, by learning only data's most obvious features. This proves useful in denoising input, for example. While the latter has the capability of capturing larger detail from the inputs, its learning converges to the identity. In a denoising scenario, this means the output will be just as noisy as the input. This is a disadvantage of using deviation from input as a criterion.

\noindent An undercomplete autoencoder essentially learns to perform dimensionality reduction. Thus, its performance can be compared with principal component analysis (PCA), or t-distributed stochastic neighbor embedding (t-SNE). In fact, an autoencoder without non-linearities is expected to learn the structure of a PCA subspace. Naturally, non-linearity aids in capturing a more diverse set of features, which arguably endows the autoencoder with a larger descriptive power. To test this, its performance may be compared to t-SNE, which is well-established as a standard for performing dimensionality reduction, also being non-linear.

\noindent The structure of a GAN differs slightly from that of an autoencoder. It starts with a generator which produces samples $\mathbf{x} = g(\mathbf{z},w_{g})$ from a randomly generated latent vector $\mathbf{z}$, where $w_{g}$ are the generator parameters. These samples are then fed into the discriminator, whose goal is to observe an input, and reach a conclusion $y = d(\mathbf{x}, w_{d}) = d(g(\mathbf{z},w_{g}), w_{d})$ about said input (with $w_{d}$ being the discriminator parameters). The learning task can be formulated as a zero-sum game solved by a minimax decision.\footnote{Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., and Bengio, Y. (2014c). Generative adversarial networks. In NIPS’2014.} The discriminator observes a payoff $v(g, d)$, while the generator receives $-v(g,d)$. Thus, the goal is to find $g^*$ such that:

\begin{equation}
    g^* = \arg\min_{g}\max_{d} v(g,d)
\end{equation}

Normally, the choice of $v(g, d)$ is:
\begin{equation}
    v_{\text{mm}}(g,d) = - \mathbb{E}_{\mathbf{x}\sim p_{\text{data}}}[\log(d(\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim p_{g}}[\log(1 - d(\mathbf{x}))]
\end{equation}

In other words, the aim of the minimax game is for the discriminator to maximize its payoff by classifying fake/real inputs correctly, while the generator attempts to minimize the discriminator's payoff by tricking it into believing that the inputs are real.

\noindent One issue with the minimax game formulation is that the gradient quickly diverges away from $d(\mathbf{x}) = 1/2$. This can be addressed by modifying the generator loss, as is done in non-saturating GAN (NSGAN) formulation.\footnote{Lucic, M., Kurach, K., Michalski, M., Bousquet, O., Gelly, S.,  (2018). Are GANs Created Equal? A Large-Scale Study. In NIPS'2018.} In this scenario, the discriminator loss is $v_{\text{mm}}$, while the generator loss is:
\begin{equation}
    v_{\text{NS}}(g,d) = - \mathbb{E}_{\mathbf{x}\sim p_{g}}[\log(d(\mathbf{x}))]
\end{equation}

Both the minimax and the non-saturating formulations will be explored.

%%%%%%%%%%%%%%  Methods section %%%%%%%%%%%%%%
\section{Methods}
The autoencoder task consists in minimizing the MSE between input and output.
\begin{equation}
\{\hat{w}\}= \arg\min_{w} \frac{1}{n}\sum_{i = 1}^{n}\left(\hat{\mathbf{x}}_i - \mathbf{x}_i\right)^{2}
\end{equation}

The model design consists of three convolutional layers, followed by an unflatten layer, and two fully connected layers. The last layers
\section{Results}
\end{document}