{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41b65fd-7bd1-47d5-9c14-724a91de23c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f33e10-3419-4993-93fc-0ee1af805d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e1701-c9ea-4b95-b1ae-48b3bcef8160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5, 0.5)]) \n",
    "\n",
    "train_dataset = datasets.FashionMNIST('classifier_data', train=True, download=True, transform = transform)\n",
    "test_dataset  = datasets.FashionMNIST('classifier_data', train=False, download=True, transform = transform)\n",
    "\n",
    "n_train = int(len(train_dataset))\n",
    "n_test = int(len(test_dataset))\n",
    "\n",
    "split_idx = int(0.75*n_train)\n",
    "train_samples = rand_sampler(list(range(n_train))[:split_idx])\n",
    "val_samples = rand_sampler(list(range(n_train))[split_idx:])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler = train_samples, batch_size = 128, num_workers = 0)\n",
    "val_dataloader = DataLoader(train_dataset, sampler = val_samples, batch_size = 128, num_workers = 0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = n_test, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e00ae9-7a6b-4213-9238-faa940ba65df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE AT INDEX 1\n",
      "LABEL: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 1\n",
    "image = train_dataset[sample_index][0].squeeze()\n",
    "label = train_dataset[sample_index][1]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.imshow(image, cmap='Greys')\n",
    "print(f\"SAMPLE AT INDEX {sample_index}\")\n",
    "print(f\"LABEL: {label}\")\n",
    "\n",
    "img_shape = tuple(image.shape)\n",
    "no_labels = len(np.unique(train_dataset.train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79241866-7070-4daf-978e-2f0f91f4a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_API(nn.Module):\n",
    "    \n",
    "#    super(nn.Module).__init__()\n",
    "    super(nn.Module).train()\n",
    "    super(nn.Module).zero_grad()\n",
    "    def train_model(self, device, train_loader, val_loader, num_epochs, loss_fn, optimizer, verbose = True):\n",
    "        train_loss_log = []\n",
    "        val_loss_log = []\n",
    "        for epoch_num in range(num_epochs):\n",
    "            ### TRAIN\n",
    "            train_loss= []\n",
    "            self.train() # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
    "            for sample_batched in train_loader:\n",
    "                # Move data to device\n",
    "                x_batch = sample_batched[0].to(device)\n",
    "                label_batch = sample_batched[1].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                out = self(x_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(out, label_batch)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Save train loss for this batch\n",
    "                loss_batch = loss.detach().cpu().numpy()\n",
    "                train_loss.append(loss_batch)\n",
    "\n",
    "            train_loss = np.mean(train_loss)\n",
    "            train_loss_log.append(train_loss)\n",
    "\n",
    "            ### VALIDATION\n",
    "            val_loss= []\n",
    "            self.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "            with torch.no_grad(): # Disable gradient tracking\n",
    "                for sample_batched in val_loader:\n",
    "                    # Move data to device\n",
    "                    x_batch = sample_batched[0].to(device)\n",
    "                    label_batch = sample_batched[1].to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    out = self(x_batch)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = loss_fn(out, label_batch)\n",
    "\n",
    "                    # Save val loss for this batch\n",
    "                    loss_batch = loss.detach().cpu().numpy()\n",
    "                    val_loss.append(loss_batch)\n",
    "\n",
    "                # Save average validation loss\n",
    "                val_loss = np.mean(val_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch: {epoch_num} :::::::::: AVERAGE VAL LOSS: {np.mean(val_loss):.5f}\", end = '\\r')\n",
    "                val_loss_log.append(val_loss)\n",
    "\n",
    "        self.train_history = train_loss_log\n",
    "        self.val_history = val_loss_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0ad9a-930f-4c5d-97c1-1795700e387c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_2912/209344839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCNN2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_API\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m         \"\"\"\n",
      "\n",
      "\u001b[0;32m/tmp/ipykernel_2912/209344839.py\u001b[0m in \u001b[0;36mCNN2D\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCNN2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_API\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv1_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m         \"\"\"\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "class CNN2D(nn.Module, train_API):\n",
    "    super(nn.Module).train()\n",
    "    super(nn.Module).zero_grad()\n",
    "    def __init__(self, conv1_size, conv2_size, Nh1, Nh2, N_labels):\n",
    "        \"\"\"\n",
    "        Ni - Input size\n",
    "        Nh1 - Neurons in the 1st hidden layer\n",
    "        Nh2 - Neurons in the 2nd hidden layer\n",
    "        No - Output size\n",
    "        \"\"\"\n",
    "        super(nn.Module).__init__()\n",
    "\n",
    "        super(train_API).train_model()\n",
    "        self.conv1 = nn.Conv2d(1, conv1_size, 5) # 28 to 24(* conv1_size)\n",
    "        self.conv2 = nn.Conv2d(conv1_size, conv2_size, 5) # 24 to 12, and 12 to 8(*conv2_size)\n",
    "        self.fc1 = nn.Linear(in_features= conv1_size*4**2, out_features=Nh1) # 8 to 4(*conv2_size), 4 to 4*4*conv2_size\n",
    "        self.fc2 = nn.Linear(in_features=Nh1, out_features=Nh2)\n",
    "        self.out = nn.Linear(in_features=Nh2, out_features=N_labels)\n",
    "                \n",
    "        print(\"Network initialized\")\n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2, stride = 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2, stride = 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.out(x))\n",
    "        return x\n",
    "    \n",
    "    def no_params_conv(self, in_size, kernel, padding, stride):\n",
    "        \"\"\"\n",
    "        Finds no of parameters per channel after every convolution/pooling\n",
    "        \"\"\"\n",
    "        return int((in_size - kernel + 2*padding)/stride + 1)\n",
    "    \n",
    "    \n",
    "#     def train_model(self, device, train_loader, val_loader, num_epochs, loss_fn, optimizer, verbose = True):\n",
    "#         train_loss_log = []\n",
    "#         val_loss_log = []\n",
    "#         for epoch_num in range(num_epochs):\n",
    "#             ### TRAIN\n",
    "#             train_loss= []\n",
    "#             self.train() # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
    "#             for sample_batched in train_loader:\n",
    "#                 # Move data to device\n",
    "#                 x_batch = sample_batched[0].to(device)\n",
    "#                 label_batch = sample_batched[1].to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 out = self(x_batch)\n",
    "\n",
    "#                 # Compute loss\n",
    "#                 loss = loss_fn(out, label_batch)\n",
    "\n",
    "#                 # Backpropagation\n",
    "#                 self.zero_grad()\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 # Update the weights\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 # Save train loss for this batch\n",
    "#                 loss_batch = loss.detach().cpu().numpy()\n",
    "#                 train_loss.append(loss_batch)\n",
    "\n",
    "#             train_loss = np.mean(train_loss)\n",
    "#             train_loss_log.append(train_loss)\n",
    "\n",
    "#             ### VALIDATION\n",
    "#             val_loss= []\n",
    "#             self.eval() # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "#             with torch.no_grad(): # Disable gradient tracking\n",
    "#                 for sample_batched in val_loader:\n",
    "#                     # Move data to device\n",
    "#                     x_batch = sample_batched[0].to(device)\n",
    "#                     label_batch = sample_batched[1].to(device)\n",
    "\n",
    "#                     # Forward pass\n",
    "#                     out = self(x_batch)\n",
    "\n",
    "#                     # Compute loss\n",
    "#                     loss = loss_fn(out, label_batch)\n",
    "\n",
    "#                     # Save val loss for this batch\n",
    "#                     loss_batch = loss.detach().cpu().numpy()\n",
    "#                     val_loss.append(loss_batch)\n",
    "\n",
    "#                 # Save average validation loss\n",
    "#                 val_loss = np.mean(val_loss)\n",
    "#                 if verbose:\n",
    "#                     print(f\"Epoch: {epoch_num} :::::::::: AVERAGE VAL LOSS: {np.mean(val_loss):.5f}\", end = '\\r')\n",
    "#                 val_loss_log.append(val_loss)\n",
    "\n",
    "#         self.train_history = train_loss_log\n",
    "#         self.val_history = val_loss_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc4c60-8840-4a5d-aab7-4e18bd764066",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_arch = [nn.Conv2d(1, 4, 5),\n",
    "            nn.Conv2d(4, 6, 5),\n",
    "            nn.Linear(6*4**2, 12),\n",
    "            nn.Linear(12, 12),\n",
    "            nn.Linear(12, 10)]\n",
    "\n",
    "forward_pass = [{'layer' : CNN_arch[0], 'function' : F.max_pool2d, 'act' : F.relu, 'args' : (2), 'kwargs' : {\"stride\" : 2}},\n",
    "                {'layer' : CNN_arch[1], 'function' : F.max_pool2d, 'act' : F.relu, 'args' : (2), 'kwargs' : {\"stride\" : 2}},\n",
    "                {'layer' : '',          'function' : torch.flatten, 'act' : '', 'args' : (1),        'kwargs' : ''},\n",
    "                {'layer' : CNN_arch[2], 'function' : '', 'act' : F.relu, 'args' : '', 'kwargs' : '' },\n",
    "                {'layer' : CNN_arch[3], 'function' : '', 'act' : F.relu, 'args' : '', 'kwargs' : '' },\n",
    "                {'layer' : CNN_arch[4], 'function' : '', 'act' : F.relu, 'args' : '', 'kwargs' : '' }]\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, additional_out=False):\n",
    "        for in_dict in self.activations:\n",
    "            if (type(in_dict['function']) != str):\n",
    "                x = in_dict['function'](in_dict['act'](in_dict['layer'](x)), *in_dict['args'], **in_dict['kwargs'])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "{'NN_layers' : [nn.Conv2d(1, 4, 5), nn.Conv2d(4, 6, 5), nn.Linear(6*4**2, 12), nn.Linear(12, 12), nn.Linear(12, 10), nn.Linear(10, 10)],\n",
    "                'c_funcs' : [F.max_pool2d, F.max_pool2d, torch.flatten, F.relu, F.relu, F.relu, F.relu],\n",
    "                'arguments' : [(F.relu, 2, 2), (F.relu, 2, 2), (1), (), (), (), ()]}\n",
    "\n",
    "\n",
    "\n",
    "# (F.relu(CNN_arch[0]), 2, stride = 2),\n",
    "#                 F.max_pool2d(F.relu(CNN_arch[0]), 2, stride = 2),\n",
    "#                 torch.flatten, F.relu, F.relu, F.relu, F.relu]\n",
    "\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), 2, stride = 2)\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), 2, stride = 2)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.relu(self.out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478421f-5d69-4c5b-93b3-eb832209206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69ba92-9f2f-4d90-8799-6acbe2dc5558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str, builtin_function_or_method, str, int, str]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forward_pass = [{'layer' : CNN_arch[0], 'function' : F.max_pool2d, 'act' : F.relu, 'args' : (2), 'kwargs' : {\"stride\" : 2}},\n",
    "                {'layer' : CNN_arch[1], 'function' : F.max_pool2d, 'act' : F.relu, 'args' : (2), 'kwargs' : {\"stride\" : 2}},\n",
    "                {'layer' : '',          'function' : torch.flatten, 'act' : '', 'args' : (1),        'kwargs' : ''},\n",
    "                {'layer' : CNN_arch[2], 'function' : '', 'act' : F.relu, 'args' : '', 'kwargs' : '' },\n",
    "                {'layer' : CNN_arch[3], 'function' : '', 'act' : F.relu, 'args' : '', 'kwargs' : '' },\n",
    "                {'layer' : CNN_arch[4], 'function' : '', 'act' : F.relu, 'args' : '', 'kwargs' : '' }]\n",
    "[type(ay) for ay in forward_pass[2].values()]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "993d0020-1162-4ea9-bb1f-88145b7d819e",
   "metadata": {},
   "source": [
    "forward_pass[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf68162-aa28-4e66-abe2-6759891ad7b0",
   "metadata": {},
   "source": [
    "For ease, I'll normalize the pixel values of the data from (0,255) to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3775f7-ccaa-4733-883d-acbffa801cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "12\n",
      "8\n",
      "4\n",
      "Network initialized\n",
      " ** On entry to SGEMM  parameter number 10 had an illegal value\n",
      "Epoch: 163 :::::::::: AVERAGE VAL LOSS: 2.30258\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_6275/588332133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfashion_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfashion_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6275/1308688975.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, device, train_loader, val_loader, num_epochs, loss_fn, optimizer, verbose)\u001b[0m\n",
      "\u001b[1;32m     54\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Training mode (e.g. enable dropout, batchnorm updates,...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# Move data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     58\u001b[0m                 \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n",
      "\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n",
      "\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n",
      "\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     96\u001b[0m         \"\"\"\n",
      "\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n",
      "\u001b[1;32m    136\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fashion_classifier = CNN2D(img_shape[0], 4, 6, 12, 12, 10, no_labels)\n",
    "\n",
    "fashion_classifier.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fashion_classifier.parameters(), lr = 0.1)\n",
    "\n",
    "fashion_classifier.train_model(device, train_dataloader, val_dataloader, 200, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6199304456057414ec3d1b68eb9961ec9761327b29f9a73dcfd441887d398725"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
